{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpE8E+jAs9IodarcF6q4Hj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liangli217/PyTorch_ML/blob/main/Multi_Headed_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "Multi-Head attention is the bread and butter of the Transformer architecture.   \n",
        "Why use multiple attention heads?\n",
        "\n",
        "\n",
        "*   Captures different relationships: different heads attend to different aspects of the input\n",
        "*   Improves learning efficiency: By operating in parallel, multiple heads allow for better learning of dependencies\n",
        "\n",
        "Your task is to code up the `MultiHeadedSelfAttention` class making use of the given `SingleHeadAttention` . The forward method should return a (batch_size, context_length, attention_dim) tensor.\n",
        "\n"
      ],
      "metadata": {
        "id": "rpzdjubiDr49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mviP4K0JDlNV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtyping import TensorType\n",
        "\n",
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim: int, attention_dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        torch.manual_seed(0)\n",
        "        # Hint: nn.ModuleList() will be useful. It works the same as a Python list\n",
        "        # but is useful here since instance variables of any subclass of nn.Module\n",
        "        # must also be subclasses of nn.Module\n",
        "\n",
        "        # Use self.SingleHeadAttention(embedding_dim, head_size) to instantiate. You have to calculate head_size.\n",
        "        pass\n",
        "\n",
        "    def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "        # Return answer to 4 decimal places\n",
        "        pass\n",
        "\n",
        "    class SingleHeadAttention(nn.Module):\n",
        "        def __init__(self, embedding_dim: int, attention_dim: int):\n",
        "            super().__init__()\n",
        "            torch.manual_seed(0)\n",
        "            self.key_gen = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
        "            self.query_gen = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
        "            self.value_gen = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
        "\n",
        "        def forward(self, embedded: TensorType[float]) -> TensorType[float]:\n",
        "            k = self.key_gen(embedded)\n",
        "            q = self.query_gen(embedded)\n",
        "            v = self.value_gen(embedded)\n",
        "\n",
        "            scores = q @ torch.transpose(k, 1, 2) # @ is the same as torch.matmul()\n",
        "            context_length, attention_dim = k.shape[1], k.shape[2]\n",
        "            scores = scores / (attention_dim ** 0.5)\n",
        "\n",
        "            lower_triangular = torch.tril(torch.ones(context_length, context_length))\n",
        "            mask = lower_triangular == 0\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "            scores = nn.functional.softmax(scores, dim = 2)\n",
        "\n",
        "            return scores @ v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Hu92elEWDrZP"
      }
    }
  ]
}